import logging
import sys
import streamlit as st
from llama_index.core.response.pprint_utils import pprint_response

from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.core.node_parser import SentenceSplitter


import os.path
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
)

# Check if storage already exists
PERSIST_DIR = "./storage"


def setup_logging():
    logging.basicConfig(stream=sys.stdout, level=logging.INFO)
    logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))


def setup_llm(api_key, azure_endpoint, api_version):
    llm = AzureOpenAI(
        model="gpt-35-turbo-16k",
        deployment_name="gpt-35-turbo",
        api_key=api_key,
        azure_endpoint=azure_endpoint,
        api_version=api_version,
    )
    return llm


def setup_embedding(api_key, azure_endpoint, api_version):
    embed_model = AzureOpenAIEmbedding(
        model="text-embedding-ada-002",
        deployment_name="text-embedding-ada-002",
        api_key=api_key,
        azure_endpoint=azure_endpoint,
        api_version=api_version,
    )
    return embed_model


def setup_index(documents):
    Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)
    index = VectorStoreIndex.from_documents(documents,transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],)
    return index


def initialize(api_key, azure_endpoint, api_version):
    setup_logging()
    logging.info("Initializing AzureOpenAI...")
    llm = setup_llm(api_key, azure_endpoint, api_version)
    logging.info("Initializing Azure OpenAI Embeddings...")
    embed_model = setup_embedding(api_key, azure_endpoint, api_version)
    Settings.llm = llm
    Settings.embed_model = embed_model
    logging.info("Done initializing both the OpenAI LLM and Embeddings...")


def document_loader(directory_path):
    try:
        if not os.path.exists(PERSIST_DIR):
            st.info("Loading documents and creating VectorStoreIndex...")
            documents = SimpleDirectoryReader(input_dir=directory_path).load_data(num_workers=4)
            index = setup_index(documents)
            st.info("Saving the VectorStoreIndex to the persistent storage...")
            index.storage_context.persist(persist_dir=PERSIST_DIR)
            st.success("Documents loaded successfully and VectorStoreIndex created.")
        else:
            st.info("Loading the existing VectorStoreIndex from the Persistent Storage...")
            storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
            index = load_index_from_storage(storage_context)
            st.success("Loaded the VectorStoreIndex successfully from the persistent storage.")
        return index
    except Exception as e:
        st.error(f"Error loading documents: {str(e)}")


def update_index(directory_path):
    try:
        st.info("Loading documents for updating the VectorStoreIndex...")
        documents = SimpleDirectoryReader(input_dir=directory_path).load_data(num_workers=4)
        index = setup_index(documents)
        st.info("Done updating the VectorStoreIndex.")
        st.info("Saving the VectorStoreIndex to the persistent storage...")
        index.storage_context.persist(persist_dir=PERSIST_DIR)
        st.success("Documents loaded and VectorStoreIndex updated successfully.")
        return index
    except Exception as e:
        st.error(f"Error updating index: {str(e)}")


def main():
    api_key = "a6efd3fae742488ebbcaefeaaddb0aff"
    azure_endpoint = "https://openai-ppcazure017.openai.azure.com/"
    api_version = "2023-03-15-preview"

    initialize(api_key, azure_endpoint, api_version)

    st.markdown(
        """
        <style>
            .sidebar .sidebar-content {
                background-color: #f0f2f6;
            }
            .reportview-container .main .block-container {
                background-color: #ffffff;
            }
            .stTextInput>div>div>textarea {
                background-color: #f8f9fa !important;
                color: #000000 !important;
                border-radius: 5px;
                width: 100%:
                height:100px;
            }
            .stButton>button {
                background-color: #007bff !important;
                color: #ffffff !important;
                border-radius: 5px;
            }
            .amswer-box{
                background-color: #f0f8ff;
                padding: 15px;
                border-radius: 10px
            }
        </style>
        """,
        unsafe_allow_html=True,
    )

    with st.sidebar:
        st.title("ðŸ“„ Menu:")
        st.sidebar.markdown("---")
        directory_path = st.text_area("Enter the directory path of files:")
        if st.button("Update Index"):
            with st.spinner("Updating Index..."):
                st.session_state.index = update_index(directory_path)
        if st.button("Submit & Process"):
            with st.spinner("Processing..."):
                st.session_state.index = document_loader(directory_path)

    st.title("ðŸ¤– RAG using Llama Index & OpenAI")

    question = st.text_area("Ask a question:", height=100, max_chars=500)

    if question:
        try:
            query_engine = st.session_state.index.as_query_engine()
            answer = query_engine.query(question)
            answer.get_formatted_sources()
            with st.container():
                st.markdown("<div class='answer-box'>", unsafe_allow_html=True)
                pprint_response(answer, show_source=True)
                st.write("Answer: \n"  , answer.response)
                st.markdown("</div>", unsafe_allow_html=True)
        except Exception as e:
            st.error(f"Error processing question: {str(e)}")


if __name__ == "__main__":
    main()
