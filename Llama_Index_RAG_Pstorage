Regarding the Azure OpenAI service, From yesterday i have been facing this issue of communication error with API "Error: (<ErrorCodes.ServiceError: 6>, 'OpenAI service failed to complete the chat', APIConnectionError(message='Error communicating with OpenAI', http_status=None, request_id=None)) 



import logging
import sys
import streamlit as st
from llama_index.core.response.pprint_utils import pprint_response

from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

import os.path
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
)

# check if storage already exists
PERSIST_DIR = "./storage"


# either way we can now query the index
# query_engine = index.as_query_engine()
# response = query_engine.query("What are transformers?")
# print(response)

def setup_logging():
    logging.basicConfig(stream=sys.stdout, level=logging.INFO)
    logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

def setup_llm(api_key, azure_endpoint, api_version):
    llm = AzureOpenAI(
        model="gpt-35-turbo-16k",
        deployment_name="gpt-35-turbo",
        api_key=api_key,
        azure_endpoint=azure_endpoint,
        api_version=api_version,
    )
    return llm

def setup_embedding(api_key, azure_endpoint, api_version):
    embed_model = AzureOpenAIEmbedding(
        model="text-embedding-ada-002",
        deployment_name="text-embedding-ada-002",
        api_key=api_key,
        azure_endpoint=azure_endpoint,
        api_version=api_version,
    )
    return embed_model

def setup_index(documents):
    index = VectorStoreIndex.from_documents(documents)
    return index

def setup_huggingface_embedding():
    embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')
    return embed_model

def initialize(api_key, azure_endpoint, api_version):
    setup_logging()
    logging.info("Initializing AzureOpenAI...")
    llm = setup_llm(api_key, azure_endpoint, api_version)
    logging.info("Initializing Embeddings Model...")
    embed_model = setup_huggingface_embedding()
    Settings.llm = llm
    Settings.embed_model = embed_model
    logging.info("Done initializing...")

def document_loader(directory_path):
    if not os.path.exists(PERSIST_DIR):
        # load the documents and create the index
        logging.info("Loading the document and creating vectorstoreindex")
        documents = SimpleDirectoryReader(input_dir= directory_path).load_data(num_workers=4)
        index = setup_index(documents)
        # store it for later
        logging.info("Saving vectorstoreindex")
        index.storage_context.persist(persist_dir=PERSIST_DIR)
        logging.info("Storing Done")
    else:
    # load the existing index
        logging.info("Loading existing Data")
        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
        index = load_index_from_storage(storage_context)
        logging.info("Loading completed")
    # documents = SimpleDirectoryReader(input_dir= directory_path).load_data(num_workers=4)
    # logging.info("Setting up VectorStoreIndex")
    # index = setup_index(documents)
    # logging.info("Done creating VectorStoreIndex")
    return index

def main():

    api_key = "c3417acc5c654125b0b74cffeea8b491"
    azure_endpoint = "https://openai-ppcazure017.openai.azure.com/"
    api_version = "2023-03-15-preview"

    initialize(api_key, azure_endpoint, api_version)

    with st.sidebar:
        st.title("ðŸ“„ Menu:")
        st.sidebar.markdown("---")
        directory_path = st.sidebar.text_input("Enter the directory path of files:")
        if st.sidebar.button("Submit & Process"):
            st.session_state.index = document_loader(directory_path)
     
    st.title("ðŸ¤–RAG using Llama Index & OpenAI")



    question = st.text_input("Enter your question:")

    if question:
        query_engine = st.session_state.index.as_query_engine()
        answer = query_engine.query(question)
        answer.get_formatted_sources()
        pprint_response(answer, show_source=True)
        st.info("Answer:", answer.response)


if __name__ == "__main__":
    main() 


TypeError: AlertMixin.info() takes 2 positional arguments but 3 were given
Traceback:
File "c:\Users\deekshith.p\Llama_Index_PDF_RAG\.venv\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 535, in _run_script
    exec(code, module.__dict__)
File "C:\Users\deekshith.p\Llama_Index_PDF_RAG\Llama_RAG_Pstorage.py", line 122, in <module>
    main()
File "C:\Users\deekshith.p\Llama_Index_PDF_RAG\Llama_RAG_Pstorage.py", line 118, in main
    st.info("Answer:", answer.response)
File "c:\Users\deekshith.p\Llama_Index_PDF_RAG\.venv\lib\site-packages\streamlit\runtime\metrics_util.py", line 397, in wrapped_func
    result = non_optional_func(*args, **kwargs)
